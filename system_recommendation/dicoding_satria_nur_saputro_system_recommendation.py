# -*- coding: utf-8 -*-
"""satria nur saputro_Dicoding_System Recomend.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16vut7K8u8RUjDBVs0t9iaKMR_fg3utb0

# Laporan Proyek Machine Learning - **Satria Nur Saputro**

## Project Overview
Domain dipilih untuk proyek ini adalah minat buku, rendahnya minat baca dikalangan masyarakat serta kurangnya bimbingan atau arahan baik orang tua , lingkup, maupun referensi buku-buku yang tidak sesuai / tidak relevan saat disajikan menjadi faktor utama kurangnya antusias pembaca, hal tersebut menjadi problem mengapa dari banyaknya buku-buku dengan berbagai genre tetapi sulit untuk masyarakat tertarik dalam membaca dan mencari minat kesukaan buku-buku yang mereka harapkan ketemu untuk dibaca, hal tersebut mendorong revosioner baru dalam bidang penjualan baik offline maupun online dalam hal offline biasanya keterbatasan ketersidiaan buku dan juga jauhnya lokasi mebuat sulit efisiensi berkurang dan juga belum lagi jika buku tersebut belum ada di toko, sedangkan online biasanya toko tersebut memiliki jangkau yang lebih luas dan memiliki buku yang lebih lengkap serta penerapan global internet yang besar, dengan kemajuan system teknologi dan juga bantuan AI dalam bidang system rekomendasi membuat ketertarian dan penyajian konten lebih cocok berdasarkan pengguna masing-masing yang diharapkan mendukung meningkatkan minat membaca

## Business Understanding
Sebagai pemilik bisnis pastinya ingin meningkatkan transaksi penjualan buku maka dari itu mereka menggunakan jasa dari Data Scientist dan ahli machine learning. Mereka ditugaskan untuk mengumpulkan data-data buku dan rating dari user terhadap antusias buku dari database sistem perusahaan. Sistem rekomendasi tidak hanya memberikan profit untuk pemilik perusahaan bisnis, akan tetapi juga memberikan profit kepada pemilik perusahaan bisnis buku lain, bisa juga meningkatkan peluang agar perusahaan buku yang kita miliki direkomendasikan kepada pembaca yang belum pernah mengunjungi toko bisnis buku yang kita miliki [1] [2].

### Problem Statements
- Dari dataset buku dan rating yang dimiliki, bagaimana perusahaan dapat memberikan rekomendasi terhadap buku yang mungkin disukai dan belum pernah dibaca oleh pembaca?

### Goals
- menerapkan sejumah rekomendasi user dari preferensi masa lalu dengan teknik *content-based filtering*
- Menghasilkan sejumlah rekomendasi buku yang sesuai dengan preferensi pengguna dan belum pernah dikunjungi sebelumnya dengan teknik *collaborative filtering*.

#### Solution statements
Solusi yang akan digunakan untuk menyelesaikan permasalahan menggunakan 3 pendekatan algoritma rekomendasi yaitu pendekatan *Cosine Similarity*, *SVD* dan *ALS*.

Penjelasan Algoritma:
- **Cosine Similarity** Cosine Similarity adalah ‘ukuran kesamaan’, salah satu implementasinya adalah pada kasus mencari tingkat kemiripan teks pada teks itu sendiri atau sentence/kalimat. Kemiripan teks bisa kita gunakan untuk membuat steganografi ataupun steganalysis linguistic [1]

- **Single Value Decomposition (SVD)**. Teknik yang banyak digunakan untuk menguraikan matriks menjadi beberapa matriks komponen, memperlihatkan banyak sifat yang berguna dan menarik dari matriks asli. Menggunakan SVD, kita dapat menentukan peringkat matriks, mengukur sensitivitas sistem linier terhadap kesalahan numerik, atau mendapatkan pendekatan peringkat rendah yang optimal untuk matriks [3].
 
- **Alternating Least Squares (ALS)**. Algoritma faktorisasi matriks dan berjalan sendiri secara paralel. ALS diimplementasikan di Apache Spark ML dan dibangun untuk masalah penyaringan kolaboratif skala besar. ALS melakukan pekerjaan yang cukup baik dalam memecahkan skalabilitas dan kelangkaan data Rating, dan ini sederhana dan berskala baik untuk kumpulan data yang sangat besar [4].

## Data Understanding
Data yang akan digunakan dalam proyek ini adalah "goodbooks", dibuat oleh Philipp Spachtholz dalam ilustrator buku. Kumpulan data ini berisi peringkat untuk sepuluh ribu buku populer. Mengenai sumbernya, katakanlah peringkat ini ditemukan di internet. Umumnya, ada 100 ulasan untuk setiap buku, meskipun beberapa memiliki peringkat yang lebih sedikit - lebih sedikit. Peringkat pergi dari satu sampai lima. Ada juga buku yang ditandai untuk dibaca oleh pengguna, metadata buku (penulis, tahun, dll.) Dan tag. Dataset terdiri dari 4 buah file csv.Pada bagian ini, kamu juga bisa menjelaskan mengenai tahapan data yang kamu lakukan saat EDA. Sumber data [5].
- book_tags.csv : terdiri dari 999913 row dan 3 kolom
- book.csv :  terdiri dari 10 ribu row dan 23 kolom 
- rating.csv : terdiri dari 981756 ribu row dan 3 kolom
- tags.csv : terdiri dari 34253 row dan 2 kolom
- to_read.csv : terdiri dari 912706 dan 2 kolom

Variabel-variabel pada book_tags.csv adalah sebagai berikut:
- goodreads_book_id : id buku yang diulas baik
- tag_id : id tag buku
- count : jumlah buku

Variabel-variabel pada book_tags.csv adalah sebagai berikut:
- goodreads_book_id : id buku yang diulas baik
- tag_id : id tag buku
- count : jumlah buku

Variabel-variabel pada tags.csv adalah sebagai berikut:
- tag_id : id tag buku
- tag_name : tag nama buku

Variabel-variabel pada to_read.csv adalah sebagai berikut:
- user_id : id user (pengguna)
- book_id : id buku

Variabel-variabel pada book.csv adalah sebagai berikut:
- id = id dari index
- 'book_id'= buku id dari buku id 
- 'best_book_id'= id buku terbaik 
- 'work_id' = id kerja buku
- 'books_count' = jumlah buku
- 'isbn' = serial nomer buku
- 'isbn13' = 13 serial nomer buku 
- 'authors' = pencipta/pembuat buku
- 'original_publication_year' = publikasi tahun buku
- 'original_title' = judul orisinil buku
- 'title' = nama judul buku
- 'language_code' = kode bahasa buku
- 'average_rating' = rating rata-rata dari user
- 'ratings_count' = jumlah rating dari user
- 'work_ratings_count' = rating kerja dari user
- 'work_text_reviews_count' = review jumlah dari user
- 'ratings_1' = jumlah rating 1 dari user
- 'ratings_2' = jumlah rating 2 dari user
- 'ratings_3' = jumlah rating 3 dari user
- 'ratings_4' = jumlah rating 4 dari user
- 'ratings_5'=  = jumlah rating 5 dari buku
- 'image_url'= link image dari buku
- 'small_image_url'= link image mini dari buku

Variabel-variabel pada rating.csv adalah sebagai berikut:
- 'book_id'= id buku dari buku
- 'user_id' = id_user dari pengguna
- 'rating' = rating user terhadap buku (1-5)


Berdasarkan dari kelima file tersebut, maka pada proyek ini hanya digunakan 2 file csv yaitu rating.csv dan book.csv. Karena sistem rekomendasi akan menggunakan rating sebagai acuannya maka kedua file akan digunakan.

### download dataset
"""

#dataset buku
!wget "https://storage.googleapis.com/kaggle-data-sets/1938/3914/compressed/books.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20221117%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20221117T014410Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=9cf11f96b545f8c98fe70bda963912c562c1390a53969bf4958da4b876fc377dee3abc6e6ae5715942dc82546fb59c556767c13340a7f2dd61376546ea70621c06777870d802eb0bf9723b18056e668fb91fbd2840d0ddfa018ae3f44095cc9a57f948e6a29c26ca1a6945d60684644f68a9c83594f3853fe5fb79089cec44a691eba45b063a475c92840195377bfab5086afe19b83a91e51b8b22b640c9d61eed61609afd804c5b05319e2ea33fee36afef9c52eba55d34b6b2c93fc44f12cc46cfb10195ec442d5368a93ca64a181c4b8d78676b1105b78fccfae9e3663c9552282111e3067bed27919cfd3253555eaefebff66f542904a6bcce4947c582a5"

#dataset rating
!wget "https://storage.googleapis.com/kaggle-data-sets/1938/3914/compressed/ratings.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20221117%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20221117T014508Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=5ee9b3118199faad8be15309f95f6b9e4681c5fd86546869ff44f8f27b9b5bd28ccd3c5a72675401abe350ff2afd8f1cf75017372f424465b2a34889c6be50ef5d0645ff729da8c3be19656253c07ed33054c2c9a4edd4fe8effcddcf955285350f78dee27e3bc1d16edd2bf3e7f8f416d56a1624e5bd31d398e3f4440164abd4ba83489189961e062e902c5bf09f402ca7004c7a29e4c56bafec93ed2ba2c8baf955603b6310e6baeec3ebfdff606ce4c7f5cdbefc247f823342aa6231eaf6a915e05fcb69e1906ba3ed6550f47a509d509465098eb860619845fd785386835117503737558439ce07e2f69b585de5d2f69d7f7a61712838ab4b3f94b00d76c"

"""### Unzip dataset

ubah terlebih dulu nama dataset menjadi sesuai dengan keperluan (book dan rating) kemudian ubah format menjadi zip
"""

!unzip '/content/book.zip'

!unzip '/content/rating.zip'

"""### membaca data"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
book = pd.read_csv('/content/books.csv',encoding = "ISO-8859-1")
rating = pd.read_csv('/content/ratings.csv')


print('Jumlah data buku unik: ', len(book.book_id.unique()))
print('Jumlah data rating unik user: ', len(rating.user_id.unique()))

"""## Data Preparation
Data preparation dilakukan untuk menganalisis dataset mentah dengan cara pengumpulan dan pembersihan sebagai penyiapan sebelum melakukan modelling:
- Melakukan Visualisasi
- melakukan concatenate
- menggabungkan dataset book dan rating data menjadi satu 
- menghapus missing value dan duplikat nilai
- mengurutkan dataframe
- menghapus simbol/karakter tidak penting

### Preprocessing

#### Visualisasi
"""

pd.Series([len(book.book_id.unique()),len(rating.user_id.unique())]).plot(kind='bar',title = ("Jumlah Data Unik"),ylabel='Jumlah',xlabel='category unik' , linestyle='-',color = ["blue","red"])

"""Gambar 1.Jumlah Data Unik

category 0 berwarna biru merupakan jumlah data unik buku dengan jumlah sekitar hampir 10 rb
category 1 berwarna merah merupakan jumlah data unik rating dengan jumlah 50 rb

#### Jumlah dataset
"""

#jumlah book.csv
book.shape

"""10 rb row dan 23 kolom"""

#jumlah rating.csv
rating.shape

"""sekitar 900 row dan 3 kolom

#### Informasi nama kolom
"""

book.columns

"""nama nama kolom yang ada di dataset buku"""

rating.columns

"""nama nama kolom yang ada di dataset rating

#### Info dataset buku
"""

book.info()

"""informasi mengenai nama kolom jumlah missing value dan type data pada dataset buku

#### info dataset rating
"""

rating.info()

"""informasi mengenai nama kolom jumlah missing value dan type data pada dataset rating"""

rating.head()

"""tabel 1. Menampilkan 5 data Rating

5 data dari dataset rating terdapat 3 kolom

#### informasi distribusi rating
"""

rating.describe()

"""tabel 2. Distribusi Nilai dataset rating
Pada row index bisa dilihat distribusi mencakup 


1.   Jumlah seluruhnya
2.   Rata-ratajumlah
3.   Standar Devisiasi
4.   Niai Minimum
5.   Nilai Quartil 1
6.   Nilai Quartil 2
7.   Nilai Quartil 3
8.   Nilai Maximum

Jumlah nilai tersebut didapatkan berdasarkan setiap kolom dari seluruh row yang ada
"""

print('Jumlah userID: ', len(rating.book_id.unique()))
print('Jumlah placeID: ', len(rating.user_id.unique()))
print('Jumlah data rating: ', len(rating))

"""#### concatenate book"""

import numpy as np
 
# Menggabungkan seluruh book_id
book_all = np.concatenate((
    book.book_id.unique(),
    rating.book_id.unique(),
))

# Mengurutkan data dan menghapus data yang sama
book_all = np.sort(np.unique(book_all))
 
print('Jumlah seluruh data book berdasarkan book_id: ', len(book_all))

"""#### merge rating

"""

# Menggabungkan dataframe rating dengan book berdasarkan nilai book_id
book_merge = pd.merge(rating, book , on='book_id', how='left')
book_merge

"""Tabel.3 Menggabungkan Dataset buku dan rating

Penggabungan dataset menggunakan join left sehingga dataset buku akan jauh dominan ketimbang rating dikarenakan acuan terhadap pencocok buku_id pada dataset buku
"""

# Cek missing value dengan fungsi isnull()
book_merge.isnull().sum()

"""jumlah nilai kosong pada dataset yang baru dimerge setiap kolomnya"""

# Menghitung jumlah kemudian menggabungkannya berdasarkan book_id
book_merge.groupby('book_id').sum()

"""Tabel 4. Groupping dataframe berdasarkan Book_id

Dataset yang telah di di merge kemudian di groupby book_id sehingga tidak acak-acak

### Preparation

#### Mengatasi Missing Value
"""

book_merge.isnull().sum()

"""jumlah nilai kosong pada dataset yang baru dimerge setiap kolomnya"""

# Membersihkan missing value dengan fungsi dropna()
all_book_clean = book_merge.dropna()
all_book_clean

"""Tabel 5. Dataframe yang telah bersih

Merupakan dataframe baru yang telah dibersihkan sehingga tidak ada lagi missing value di antara row dan kolom dataset
"""

# Mengecek kembali missing value pada variabel all_book_clean
all_book_clean.isnull().sum()

"""jumlah nilai kosong pada dataset yang baru dimerge setiap kolomnya

#### Sort id book dari yang terkecil
"""

# Mengurutkan book berdasarkan book_id kemudian memasukkannya ke dalam variabel fix_book
fix_book = all_book_clean.sort_values('book_id', ascending=True)
fix_book

"""tabel 6. Urutan terkecil Dataframe

Mengurutkan nilai book_id dari yang terkecil ke yang terbesar berdasarkan kolom book_id sehingga dapat dilihat dari 1 sampai nilai terbesar urutannya

#### jumlah book id unik
"""

# Mengecek berapa jumlah fix_book
len(fix_book.book_id.unique())

"""Jumlah buku_id yang unik dari dataframe yang telah dibersihkan

#### sorting book id lalu masukann ke variabel baru
"""

# Membuat variabel preparation yang berisi dataframe fix_book kemudian mengurutkan berdasarkan book_id
preparation = fix_book
preparation.sort_values('book_id')

"""tabel 7. Urutan terkecil Dataframe

Mengurutkan nilai book_id dari yang terkecil ke yang terbesar berdasarkan kolom book_id sehingga dapat dilihat dari 1 sampai nilai terbesar urutannya yang dimasukkan kedalam variable baru

#### hapus duplicate id book
"""

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('book_id')
preparation

"""tabel 8. Dataframe Bersih

Merupakan Dataframe yang telah diurutkan secara Ascending dan telah bersih tidak ada duplikat data dan missing value
"""

preparation['title'].head(10)

"""10 data series yang ditampilkan dari kolom title"""

# Mengonversi data series ‘book_id’ menjadi dalam bentuk list
book_id = preparation['book_id'].tolist()
 
# Mengonversi data series ‘author’ menjadi dalam bentuk list
book_authors = preparation['authors'].tolist()
 
# Mengonversi data series ‘original_title’ menjadi dalam bentuk list
book_title = preparation['original_title'].tolist()
 
print(len(book_id))
print(len(book_authors))
print(len(book_title))

"""jumlah Buku id, authors, title dengan kata lain jumlah row setiap kolom semua jumlahnya sama"""

# Membuat dictionary untuk data ‘book_id’, ‘book_name’, dan ‘book_title’
book_new = pd.DataFrame({
    'id': book_id,
    'authors': book_authors,
    'title': book_title
})
book_new

"""tabel 9. Dataframe Baru

Pada Dataframe ini diambil dari data yang telah bersih kemudian hanya diambil kolom id, authors dan juga title

#### hapus regex
"""

import re
# case folding
def casefolding(text):
  text = text.lower()                                 # Ubah jadi lowercase
  text = re.sub(r'https?://\S+|www\.\S+', '', text)   # Menghapus URL
  text = re.sub(r'[-+]?[0-9]+', '', text)             # Menghapus karakter angka
  text = re.sub(r'[^\w\s]', '', text)                 # Menghapus karakter tanda baca
  text = text.strip()
  return text

# Buat fungsi untuk menggabungkan seluruh langkah pada text preprocessing
def data_preprocessing(text):
  text = casefolding(text)
  return text

book_new['authors'] = book_new.authors.apply(data_preprocessing)
book_new['title'] = book_new.title.apply(data_preprocessing)
book_new.head()

"""Tabel 10. Dataframe regex

merupakan dataframe yang telah dibersihkan antaralain simbol, lowercase dan juga angka pada kolom authors sehingga nilai pada kolom bersih hanya merupakan aplhabet

## Modeling and Results
**Alur penyelesaian Model content-based filtering dan collaborative filtering**

Pada proyek ini akan menggunakan satu algoritma untuk teknik content-based filtering:
>1.  Consine Similarty

- Penyelesaian sistem rekomendasi menggunakan Consine_similarty:

  ->  Melakukan TF-IDF terhadap dataset yang telah dilakukan preparation

  -> Menmasukkan nilai TF-IDF kedalam algoritma consine similarty

   -> melakukan prediksi 5 top rekomendasi buku

  -> evaluasi hasil menggunakan metrik presisi

  -> Setelah dilakukan prediksi pada model content-based filtering 
menggunakan data test didapatkan hasil Precision 80%



Pada proyek ini akan menggunakan dua algoritma untuk teknik collaborative filtering, yaitu:
>1. Singular Value Decomposition (SVD)
>2. Alternating Least Squares (ALS)

- Penyelesaian sistem rekomendasi Collaborative filtering:

  -> Menginstall libarary suprise

  -> Membagi data train dan test (80:20)

  -> Melatih model SVD dan ALS

  -> Evaluate hasil model menggunakan MAE dan MSE

  -> Setelah dilakukan prediksi menggunakan data test didapatkan hasil Metrik Evaluasi setiap algoritma:

    -> Singular Value Decomposition (SVD):

  MSE: 0.3276

  MAE: 0.4236

    -> Alternating Least Squares (ALS):

  MSE: 0.6287

  MAE: 0.6215

Berdasarkan kedua algoritma tersebut, maka model yang terbaik adalah Alternating Least Squares (SVD). 

**hasil top-N rekomendasi**
Hasil rekomendasi judul buku untuk top 5 terbaik dengan menggunakan algoritma Consine similarty berdasarkan authors james frey adalah
1. little women
2. a little princess
3. little town on the prairie
4. the first four years
5. little house in the big woods

Hasil rekomendasijudul buku untuk top 10 terbaik dengan menggunakan algoritma SVD berdasarkar user 23 adalah
1. City of Bones (The Mortal Instruments, #1)
2. Monster Hunter International (Monster Hunter I...
3. Disgrace
4. Half of a Yellow Sun
5. Animal, Vegetable, Miracle: A Year of Food Life
6. To All the Boys I've Loved Before (To All the ...203    
7. A Christmas Carol
8. The Fires of Heaven (Wheel of Time, #5)
9. Reached (Matched, #3)
10. The 7 Habits of Highly Effective People: Power...

Hasil rekomendasi judul buku untuk top 10 buku terbaik dengan menggunakan algoritma ALS berdasarkan user 23 adalah
1. What If?: Serious Scientific Answers to Absurd...
2. The Red Pyramid (Kane Chronicles, #1)
3. Artemis Fowl (Artemis Fowl, #1)
4. The Crucible
5. Girl with a Pearl Earring
6. Black Rose (In the Garden, #2)
7. Angels & Demons  (Robert Langdon, #1)
8. All Together Dead (Sookie Stackhouse, #7)
9. Predictably Irrational: The Hidden Forces That...
10. Library of Souls (Miss Peregrine's Peculiar Ch...

### Content Based Filtering
"""

data =book_new
data.sample(5)

"""tabel 11. menampilkan 5 dataframe baru

dataframe yang telah di lakukan proses regex kemudian ditampilkan 5 data teratas

#### TF-IDF Vectorizer

Melakukan tf-idf pada kolom title
"""

from sklearn.feature_extraction.text import TfidfVectorizer
 
# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data title
tf.fit(data['title']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

"""melakukan tranformasi bentuk matrix"""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['title']) 
 
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""ubah vektor tf-idf ke dalam bentuk matrix"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""buat dataframe baru sesuai kecocokan nilai matrix"""

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan title
# Baris diisi dengan nama authoe
 
pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.authors
).sample(22, axis=1).sample(10, axis=0)

"""tabel 12. Matrix TF IDF

merupakan matrix dengan nilai kesesuain antara title dan authors sehingga nantinya jika memiliki kecocokan maka nilainya akan 1 dan jika tidak nilainya akan 0

#### cosine similarity

Menghitung cosine similarity pada matrix tf-idf
"""

from sklearn.metrics.pairwise import cosine_similarity
 
# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""nilai consine similirity berdasarkan nilai tf idf

Membuat dataframe sesuai nilai consin
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama authors
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['authors'], columns=data['authors'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap authors
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""tabel 13. Nilai dataframe Consine similarity

merupakan matrix dengan nilai kesesuain antara title dan authors sehingga nantinya jika memiliki kecocokan maka nilainya akan hingga 1 dan jika tidak ada hubungannya nilainya akan mencapai 0

#### Top 5 rekomendasi
"""

def book_recommendations(nama_book, similarity_data=cosine_sim_df, items=data[['authors', 'title']], k=5):
    """
    Rekomendasi book berdasarkan kemiripan dataframe
 
    Parameter:
    ---
    nama_book : tipe data string (str)
                Nama bookran (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan book sebagai 
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---
 
 
    Pada index ini, kita mengambil k dengan nilai similarity terbesar 
    pada index matrix yang diberikan (i).
    """
 
 
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_book].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop nama_book agar nama book yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_book, errors='ignore')
 
    return pd.DataFrame(closest).merge(items).head(k)

"""Mendapatkan rekomendasi buku yang mirip dengan james frey"""

data[data.authors.eq('james frey')]

"""tabel 13. data yang akan diprediksi

merupakan dataset acak dengan index 101 dan authors adalah james frey

Top 5 rekomedasi
"""

# Mendapatkan rekomendasi buku yang mirip dengan james frey
book_recommendations('james frey')

"""tabel 14. 5 top teratas rekomendasi prediksi content based filtering

dataframe diatas merupakan top 5 bedasarkan authors james frey dengan pendekatan algoritma consine similarty dengan rekomendasi terbaik buku dengan judul little women

#### Evaluate

Evaluate Content Based-Filtering menggunakan Precision

Precision = #of recommended that are
Relevant = #of item we recommended
Precision rumus : 
P = P/R

Maka
P = 4/5

jadi Presisinya adalah 80%

maka dikatakan cukup baik

### Collaborative Filtering

#### Install libary
"""

!pip install surprise

"""Import Library"""

from surprise import SVD
from surprise import Reader
from surprise import Dataset
from surprise import accuracy
from surprise import BaselineOnly

import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

"""Membaca file csv"""

ratings = pd.read_csv('/content/ratings.csv')
ratings

"""tabel 15. Dataframe rating

menampilkan seluruh data pada rating dengan jumlah row sekitar 900 rb dan 3 kolom

#### mambagi data train dan test

Membagi data train dan dataset ke 80:20
"""

train_data, test_data = train_test_split(ratings, test_size = 0.2,random_state=123)

"""Memanggil data dari dataframe"""

# Dataset to suprise
reader = Reader(rating_scale = (1, 5))
data_train = Dataset.load_from_df(train_data[['book_id', 'user_id', 'rating']], reader)
data_test = Dataset.load_from_df(test_data[['book_id', 'user_id', 'rating']], reader)

"""buat full trainset"""

# buat full trainset
data_train = data_train.build_full_trainset()
data_test = data_test.build_full_trainset()

# hitung mean global data train
mean = data_train.global_mean
print('Train rating', mean)

# hitung mean global data test
mean = data_test.global_mean
print('Test rating', mean)

# buat data train dan test
data_trainset = data_train.build_testset()
data_testset = data_test.build_testset()

"""Algoritma ALS"""

# algoritma SVD 
k_factors = 5
algo = SVD(n_factors= k_factors, n_epochs= 200, biased= True, lr_all= 0.005, reg_all= 0, init_mean= 0, init_std_dev= 0.01)

# Latih algortma SVD
algo.fit(data_train)

#Test algorima SVD
predictionsSVD = algo.test(data_trainset)

"""algoritma ALS"""

bsl_options = {'method': 'als',
                 'n_epochs': 5,
                 'reg_u': 12,
                 'reg_i': 5
                }        
als = BaselineOnly(bsl_options=bsl_options)
als.fit(data_train)
predictionsALS = als.test(data_trainset)

"""#### Evaluate"""

eval = pd.DataFrame(columns=['MSE', 'MAE'], index=['SVD', 'ALS'])
model_dict = {'SVD': predictionsSVD, 'ALS': predictionsALS}
 
for name, model in model_dict.items():
    eval.loc[name, 'MSE'] = accuracy.mse(model)
    eval.loc[name, 'MAE'] = accuracy.mae(model)

"""Nilai MSE, MAE pertama merupakan evaluasi dari algoritma SVD dan MSE , MAE kedua merupakan evaluasi dari algoritma ALS

#### Prediction
"""

# Show first 5 rows
train_data.head(5)

"""tabel 16. 5 data teratas train_data

menampilkan 5 data teratas untuk pelatihan model SVD dan ALS
"""

test_data.head()

"""tabel 17. 5 data teratas test_data

menampilkan 5 data teratas untuk testing model SVD dan ALS
"""

# Prediction without real rating
p1 = algo.predict(uid = train_data.iloc[1].book_id, iid = train_data.iloc[1].user_id, verbose = True)

"""data series percobaan untuk testing salah satu data index 1 pada dataset train untuk diprediksi modelnya oleh algoritma SVD"""

book

"""tabel 17. dataframe buku

menampilkan seluruh dataframe dataset buku dengan 10 ribu row dan 23 kolom

##### top-10 rekomedasi SVD predict
"""

def generate_recommendation(model, user_id, ratings_df, movies_df, n_items,book_new):
   # Get a list of all movie IDs from dataset
   movie_ids = ratings_df["book_id"].unique()
 
   # Get a list of all movie IDs that have been watched by user
   movie_ids_user = ratings_df.loc[ratings_df["user_id"] == user_id, "book_id"]
    # Get a list off all movie IDS that that have not been watched by user
   movie_ids_to_pred = np.setdiff1d(movie_ids, movie_ids_user)
 
   # Apply a rating of 4 to all interactions (only to match the Surprise dataset format)
   test_set = [[user_id, movie_id, 4] for movie_id in movie_ids_to_pred]
 
   # Predict the ratings and generate recommendations
   predictions = model.test(test_set)
   pred_ratings = np.array([pred.est for pred in predictions])
   print("Top {0} item recommendations for user {1}:".format(n_items, user_id))
   # Rank top-n movies based on the predicted ratings
   index_max = (-pred_ratings).argsort()[:n_items]
   for i in index_max:
       movie_id = movie_ids_to_pred[i]
       print(book_new.title[movies_df[movies_df["book_id"]== movie_id]["user_id"].values[0] == book_new['id']])
 
 
# define which user ID that we want to give recommendation
userID = 23
# define how many top-n movies that we want to recommend
n_items = 10
# generate recommendation using the model that we have trained
generate_recommendation(algo,userID,ratings,df,n_items,book)

"""tabel 18. 10 top rekomendasi model SVD user 23

hasil 10 top rekomendasi berdasarkan prediksi user 23 dengan model SVD bahwa judul terbaik yang sangat direkomendasikan adalah What City of Bones (The Mortal Instruments, #1)

##### top-10 rekomendasi ALS predict
"""

# define which user ID that we want to give recommendation
userID = 23
# define how many top-n movies that we want to recommend
n_items = 10
rekomendasi
generate_recommendation(als,userID,ratings,df,n_items,book)

"""tabel 19. 10 top rekomendasi model ALS user 23

hasil 10 top rekomendasi judul buku berdasarkan prediksi user 23 dengan model ALS terlihat bahwa judul terbaik yang sangat direkomendasikan adalah What If?: Serious Scientific Answers to Absurd...

## Evaluation
Evaluasi akan dilakukan dengan menggunakan kedua algoritma yaitu, Precision, Singular Value Decomposition (SVD), Alternating Least Squares (ALS).
 
Evaluasi Pertama Content-Based Filtering yaitu Precision.
-   
    Precision = #of recommended that are

    Relevant = #of item we recommended
    
    Precision rumus 
     
    P = $$P/{R}$$
    
    Maka
    P = 4/5
    jadi Presisinya adalah 80%

Evaluasi Pertama Collaborative Filtering yaitu Singular Value Decomposition (SVD).
Metrik yang digunakan untuk evaluasi dari model yaitu:
- MSE
  merepresentasikan rata-rata Kesalahan kuadrat diantara nilai aktual dan nilai peramalan. Nilai aktual adalah nilai rating, sedangkan nilai prediksi adalah estimasi dari nilai aktual.
  
  MSE = $$\sum_{i=1}^n \frac{(nilaiaktual_i-nilaiprediksi_i)^2}{n}$$
  MSE =  0.3276
 
- MAE
  merepresentasikan rata – rata kesalahan (error) absolut antara hasil peramalan dengan nilai sebenarnya. Nilai aktual adalah nilai rating, sedangkan nilai prediksi adalah estimasi dari nilai aktual.
 
  MAE = $$\sum_{i=1}^n \frac{|nilaiaktual_i-nilaiprediksi_i|}{n}$$
  MAE = 0.4236

 
Evaluasi kedua Collaborative Filtering yaitu Alternating Least Squares (ALS).
Metrik yang digunakan untuk evaluasi dari model yaitu:
- MSE
  merepresentasikan rata-rata Kesalahan kuadrat diantara nilai aktual dan nilai peramalan. Nilai aktual adalah nilai rating, sedangkan nilai prediksi adalah estimasi dari nilai aktual.
  
  MSE = $$\sum_{i=1}^n \frac{(nilaiaktual_i-nilaiprediksi_i)^2}{n}$$
  MSE = 0.6287
 
- MAE
  merepresentasikan rata – rata kesalahan (error) absolut antara hasil peramalan dengan nilai sebenarnya. Nilai aktual adalah nilai rating, sedangkan nilai prediksi adalah estimasi dari nilai aktual.
 
  MAE = $$\sum_{i=1}^n \frac{|nilaiaktual_i-nilaiprediksi_i|}{n}$$
  MAE = 0.6215

## Kesimpulan 
Pada algoritma consine similarty menunjukkan presisi yang jauh lebih akurat hingga 80% sedangkan untuk algoritma SVD dan ALS dengan evaluasi MAE dan MSE, algoritma SVD jauh lebih baik dalam memberikan prediksi rekomendasi dengan tingkat 0,4. Dalam project ini dapat dikatakan bahwa teah mencapai goal yang ditetapkan menggunakan penerapan model Content-based filtering dan Collaborative Filtering

## REFERENCES
[1]	T. Badriyah, I. Restuningtyas, and F. Setyorini, “Sistem Rekomendasi Collaborative Filtering Berbasis User Algoritma Adjusted Cosine Similarity,” Pros. Semin. Nas. Sisfotek, vol. 10, no. 1, pp. 38–45, 2021.

[2]	A. S. N. S. Ningrum, “Content Based Dan Collaborative Filtering Pada Rekomendasi Tujuan Pariwisata Di Daerah Yogyakarta,” Telematika, vol. 16, no. 1, p. 44, 2019, doi: 10.31315/telematika.v16i1.3023.

[3]	MIT, “Singular Value Decomposition (SVD) tutorial,” MIT. https://www.ptonline.com/articles/how-to-get-better-mfi-results (accessed Nov. 30, 2022).

[4]	Apache, “Apache Flink 1.2 Documentation: Alternating Least Squares.” https://nightlies.apache.org/flink/flink-docs-release-1.2/dev/libs/ml/als.html (accessed Nov. 30, 2022).

[5]	P. Spachtholz, “goodreads - content based book recommendation | Kaggle,” Kaggle, 2018. https://www.kaggle.com/code/bshirude2/goodreads-content-based-book-recommendation/data (accessed Nov. 30, 2022).
"""