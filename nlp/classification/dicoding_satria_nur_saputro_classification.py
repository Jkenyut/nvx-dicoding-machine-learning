# -*- coding: utf-8 -*-
"""Dicoding NLP intermediate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jW0MGzIUmM0NkJbhrs74zlB0jMvRZRne

## Satria Nur Saputro
# Dicoding NLP
# Dataset [Click me](https://www.kaggle.com/datasets/mrutyunjaybiswal/iitjee-neet-aims-students-questions-data?resource=download)
"""

!wget "https://storage.googleapis.com/kaggle-data-sets/957852/1700219/compressed/subjects-questions.csv.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220911%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220911T053132Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=595ed22dd9902f4cd1bdee94369d6ef6e007cad9e58030a061b6ac207fe4155a7a21da0b035742a16676cdc1788b59d27cb4c66e46fc553c3a082817331f8f6cb1e01d1bb05f89b182a6f663122c0f06aa3c53936b6767f2404c5de15b5009eefaebc59d58b667c3c0c21960e26e2308e74536f374df4506210662735882ca5007e428c7720865b3500f72653ca47fdddfbedb9d98bd14d2c040a6b0834bc8283cd5501bc4ebc756ddf45e6b498bf6d6ffd95b25d7d77c8f563a5d2befeb295f8c18acaa2ce4fa45793fb8cd174e48241221d1bc8a33c76309dfb52d7831b10e71a4f3c5040021d02c0817308ae7410f78643d3c3744119ae96189ab2ff6fdd4"

"""ubah nama file download menjadi data.zip"""

!unzip "data.zip"

"""Import library"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('/content/subjects-questions.csv')
dataset.head()

"""bar plot perCategory"""

kategori_dasatet = dataset.Subject.value_counts()
kategori_dasatet

bar_dataset = pd.DataFrame(kategori_dasatet,columns=["Subject"])
bar_dataset.plot.bar(y="Subject")

"""Convert Category Subject"""

category = pd.get_dummies(dataset.Subject)
dataset_baru = pd.concat([dataset, category], axis=1)
dataset_baru = dataset_baru.drop(columns='Subject')
dataset_baru

"""# catatan
Karena data merupakan rumus dan spasial penting maka hanya ubah ke lowercase
"""

eng = dataset_baru['eng'].str.lower()
label = dataset_baru[['Biology', 'Chemistry', 'Maths','Physics']].values

"""Jumlah kata yang ada di dataset dihitung dari per spasi"""

jumlah_kata = dataset.eng.str.split()
len(jumlah_kata)

"""# train test split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(eng, label, test_size=0.2)

print('data training ',len( X_train))
print('target training ',len( y_train))
print('data testing ',len( X_test))
print('target testing ',len( y_test))

"""# Tokenization"""

from tensorflow.keras.preprocessing.text import Tokenizer

     
tokenizer = Tokenizer(num_words=39433+1, oov_token='x')
tokenizer.fit_on_texts(X_train) 
    
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

word_indexy = tokenizer.word_index
print(len(tokenizer.word_counts))
print(tokenizer.word_counts)
# print(word_indexy)
# print(sequences_test)

"""# Padding"""

from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_train = pad_sequences(sequences_train, maxlen=1054,  padding='post') 
padded_test = pad_sequences(sequences_test,  maxlen=1054, padding='post')
print(padded_train.shape)
print(padded_test.shape)

"""Model"""

from tensorflow import keras

from keras.models import Sequential
from keras.layers import Dense, Embedding, Bidirectional, TimeDistributed, LSTM, Dropout
model = Sequential()

model = keras.Sequential()

model.add(Embedding(input_dim=39433 + 1,output_dim=16))
model.add(Bidirectional(LSTM(64)))
model.add(Dropout(0.2))
model.add(Dense(8, activation='relu'))
model.add(Dense(4, activation='softmax'))

"""# Optimizer and loss"""

optimizers = keras.optimizers.Adam(1e-3)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizers,
              metrics=['accuracy'])

model.summary()

"""# callback"""

class callback(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.92):
      self.model.stop_training = True

callbacks = callback()
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',mode="max", patience=3)

"""# Training data"""

num_epochs = 200
history = model.fit(padded_train, y_train, epochs=num_epochs, callbacks=[callbacks,early_stopping],
                    validation_data = (padded_test, y_test), batch_size=4,verbose=2)

"""# Plot akurasi dan loss"""

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()