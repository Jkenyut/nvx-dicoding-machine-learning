# -*- coding: utf-8 -*-
"""Dicoding_satria nur saputro_PA_MT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rXjAz4LIc7_-rcR6jrMFKjxdKr9ACWOY

# Dicoding Academy 


**Satria nur saputro**<br>
**satrianursaputro06@gmail.com**


Lakukan proses di atas menggunakan dataset review product https://drive.google.com/file/d/1qn5WXp-H95_FL_Rx5oqvfZaflYdHsnrF/view?usp=sharing


**Permasalahan**:
melakukan sentiment analysis salah satu aplikasi dari marketplace app pada review-review product.

**! Ingat**<br>
Kolom Ratings adalah target.

**solusi**: menerapkan classification untuk mengetahui feedback konsumen dalam mengkategorikan rating 1-5

#Domain Proyek

Melakukan melakukan sentiment analysis salah satu aplikasi dari marketplace app pada review-review product, dikarenakan sulitnya seorang bisnis dalam mendapatkan feedback atau saran dalam product yang dibuat dari mulai prasana tahap pembuatan sampai diterima ketangan konsumen, perlu adanya sebuah solus agar bisnis lebih cepat untuk mendapatkan insight lebih feedback feedback yang diterima

#Business Understanding

##Permasalahan:


1.   Kenapa pentingnya AI diterapkan pada masalah ini ?
2.   Bagaimana memahami pentingnya feedback pengguna ?
3.   Apa dampak rating dalam product ?

##Goal:



1.   Bidang AI diterapkan dalam permasalahan ini yaitu kemudahan dalam mengkategorikan sebuah feedback sehingga sebuah perusahaan dapat ber-intropeksi dan mengedepankan inovasi serta pelayanan lebih baik
2.   Sebuah feedback dari konsumen merupakan sebuah data atau saran/review product yang kita punya, sehingga kita lebih bisa mengetahui apa saja kekurangan & kelebihan produk kita serta melihat pasar konsumen apa yang mereka mau dari berbagai inovasi product yang kita sediakan untuk dibeli konsumen
3.   Pentingnya sebuah rating yaitu kualitas kepercayaan konsumen semakin tinggi, validasi sebuah brand, dan peningkatan penjualan terhadap seo sebuah marketplace


##Solution statements
 
 

1.   Mencoba melakuakan Feature TF-IDF, BoW, Feature selection
2.   Menggunakan finetunning sebuah model dengan algoritma SGD
3.   Melakukan validasi menggunakan random shufflesplit dan cross_validation

#Data Understanding

##variabel


itemId : id produk

category: category produk

name : nama produk	

rating: rating produk

originalRating : rating semula

reviewTitle : judul review

reviewContent : isi review

likeCount : jumlah suka product

upVotes : votes produk	
downVotes : votes produk

helpful: boolen manfaat produk
relevanceScore : score relevan product
boughtDate : tanggal penjualan
clientType : jenis kliean
retrievedDate : tanggal diterima

##catatan
tahap yang digunakan dalam masalah ini hanya menggunakan variabel rating dan review content

#Data Preparation

-melakukan data akusisi

-melakukan cleansing data

-melakukan preprocessing : 

1.   casefolding
2.   normalisasi
3.   stopword

-feature engginering

1.   TF-IDF
2.   Bow
3.   Feature Selection

#Modeling


kelebihan SGD: 


1.   Cocok untuk masalah classification
2.   Digunakan untuk masalah supervised
3.   algoritma sederhana cepat
4.   bagus untuk untuk data tidak begitu banyak


kekurangan SGD:


1.   machine learning tradisional
2.   jika banyak data akan kesulitan dalam mencapai akurasi optimal
3.   beberapa masalah data tidak dapat dikategorika algoritma ini bagus dengan algoritma yang lain




menggunakan algoritma SGD dengan fine tunning 

*   loss="hinge", penalty="l2", max_iter=5

serta split dataset 80%:20%, training dan testing

# Evaluasi

menggunakan 

1.   confusion matrix
2.   recall
3.   precision
4.   f1


Juga shufflesplit dan cross_validation untuk menguji accuracy model

# 1. Import library
"""

#install library sastrawi
!pip install Sastrawi

# Commented out IPython magic to ensure Python compatibility.
#memanggil library
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import re
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import SelectKBest 
from sklearn.feature_selection import chi2 
# Untuk mempermudah, simpan setiap objek agar dapat digunakan untuk pemodelan maupun deployment. Gunakan library Pickle
import pickle
import seaborn as sns

# %matplotlib inline

import nltk #libray stopwords
nltk.download('stopwords')

"""# 2. Data Acquisition"""

from google.colab import drive #connect ke g-drive
drive.mount('/content/drive')

#membaca dataset
df = pd.read_csv('/content/drive/MyDrive/Orbit/20191002-reviews.csv')

df.head() #membaca dataset top 5 dari atas

df.info() #informasi dataset

df.isna().sum().plot(kind='barh') #visualisasi jumlah bar horizontal file null

"""# 3. Data Cleansing"""

# memilih kolom rating dan reviewContent
df_new = df[['rating', 'reviewContent']]
df_new #menampilkan dataset

# drop nilai nan
df_new.dropna(inplace=True)
df_new.head() #top 5 teratas

df_new_cut = df_new.sample(5000) #melakukan sampeling 5000 data

print('Total Data:', df_new_cut.shape[0], 'data\n')
print('terdiri dari (rating):')
print('-- [1] Data yang pertama\t:', df_new_cut[df_new_cut.rating == 1].shape[0], 'data')
print('-- [2] Data yang kedua\t        :', df_new_cut[df_new_cut.rating == 2].shape[0], 'data')
print('-- [3] Data yang ketiga\t        :', df_new_cut[df_new_cut.rating == 3].shape[0], 'data')
print('-- [4] Data yang ketiga\t        :', df_new_cut[df_new_cut.rating == 4].shape[0], 'data')
print('-- [5] Data yang ketiga\t        :', df_new_cut[df_new_cut.rating == 5].shape[0], 'data')

df_new_cut.describe() #dataset statistik

plt.title('Kategori') #visualisasi judul
sns.countplot(df_new_cut.rating) #gruping kategori data per rating

"""# 4. Data Preprocessing

## 4.1 Casefolding
"""

# case folding
def casefolding(text):
  text = text.lower()                                 # Ubah jadi lowercase
  text = re.sub(r'https?://\S+|www\.\S+', '', text)   # Menghapus URL
  text = re.sub(r'[-+]?[0-9]+', '', text)             # Menghapus karakter angka
  text = re.sub(r'[^\w\s]', '', text)                 # Menghapus karakter tanda baca
  text = text.strip()                                 #menghapus spasi berlebih
  return text #return text

"""## 4.2 Normalisasi"""

# Download corpus singkatan
!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/key_norm.csv

#import data
key_norm = pd.read_csv('key_norm.csv')
#fungsi word normalization
def text_normalize(text):
  text = ' '.join([key_norm[key_norm['singkat'] == word]['hasil'].values[0] if (key_norm['singkat'] == word).any() else word for word in text.split()])
  text = str.lower(text)
  return text

"""## 4.3 Stopword Removal"""

stopwords_id = stopwords.words('indonesian') #stopword indo

# Buat stopwords removal
def remove_stopwords(text):
  clean_word = []
  all_text = text.split()
  for word in all_text:
    if word not in stopwords_id:
      clean_word.append(word)
  return ' '.join(clean_word)

"""## 4.4 Stemming"""

#deklarasi steam
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# stemming
def stemming(text):
  text = stemmer.stem(text)
  return text

"""## 4.5 Memanggil semua fungsi"""

# Buat fungsi untuk menggabungkan seluruh langkah pada text preprocessing

def data_preprocessing(text):
  text = casefolding(text)
  text = text_normalize(text)
  text = remove_stopwords(text)
  text = stemming(text)
  return text

# Commented out IPython magic to ensure Python compatibility.
# %%time
# df_new_cut['cleanReview'] = df_new_cut['reviewContent'].apply(data_preprocessing)
# # Perhatikan waktu komputasi ketika proses text preprocessing (CPU)

df_new_cut #melihat dataset

df_new_cut.to_csv('/content/drive/MyDrive/Orbit/result_contentReviewsample5000.csv', index=False) #save csv setelah pre processing

"""# 5. Feature Engineering"""

#memilih kolom
df_new_cut = pd.read_csv('/content/drive/MyDrive/Orbit/result_contentReviewsample5000.csv')
X = df_new_cut['cleanReview'].values.astype('U')#ubah type unicode
y = df_new_cut['rating']

"""## 5.1 TF-IDF
Proses mengubah teks menjadi vector menggunakan metode TF-IDF n_gram looping 3
"""

#proses tf-idf dengan n_gram 3x iterasi
res_tfidf = {
    'n_gram': [],
    'sum_token': [],
    'X_tf_idf': [],
    'tf_idf': []
    }
n = 4
for i in range(1, n):
  tf_idf = TfidfVectorizer(ngram_range=(i, i))
  tf_idf.fit(X)
  X_tf_idf = tf_idf.transform(X).toarray()
  res_tfidf['n_gram'].append(i)
  res_tfidf['sum_token'].append(len(tf_idf.get_feature_names_out()))
  res_tfidf['X_tf_idf'].append(X_tf_idf)
  res_tfidf['tf_idf'].append(tf_idf)

res_tfidf #menampilkan hasil

#Save vectorizer.vocabulary_
pickle.dump(vec_TF_IDF.vocabulary_,open("/content/drive/MyDrive/Orbit/feature_tf-idf_ngram3.pkl","wb"))

tf_idf.vocabulary_ #tampilkan vocab

#Melihat Jumlah Fitur
print(len(tf_idf.get_feature_names()))
#Melihat fitur-fitur apa saja yang ada di dalam corpus kita
print(tf_idf.get_feature_names())

#Lihat data tabular yang menggunakan metode TF-IDF
#Data ini siap untuk dimasukkan dalam proses machine learning

x1 = tf_idf.transform(X).toarray()
data_tabular_tf_idf= pd.DataFrame(x1,columns=tf_idf.get_feature_names())
data_tabular_tf_idf

"""## 5.2 Bag of Words
Proses  metode bag of words n_gram looping 3
"""

#proses BoW dengan n_gram 3x iterasi
res_bow = {
    'n_gram': [],
    'sum_token': [],
    'X_bow': [],
    'bow': []
    }
n = 4
for i in range(1, n):
  bow = CountVectorizer(ngram_range=(i,i))
  bow.fit(X)
  X_bow = bow.transform(X).toarray()
  res_bow['n_gram'].append(i)
  res_bow['sum_token'].append(len(bow.get_feature_names_out()))
  res_bow['X_bow'].append(X_bow)
  res_bow['bow'].append(bow)

res_bow #menampilan hasil

"""## 5.3 Feature selection"""

#Mengubah nilai data tabular tf-idf & y dari dataframe menjdi array agar dapat dijalankan pada proses seleksi fitur
#menggunakan n_gram 1
X_tfidf = np.array(res_tfidf['X_tf_idf'][0])
y = np.array(y)

# Ten features with highest chi-squared statistics are selected 
chi2_features = SelectKBest(chi2, k=3000) 
X_kbest_features = chi2_features.fit_transform(X_tfidf, y)

# Reduced features 
print('Original feature number:', X_tfidf.shape[1]) 
print('Reduced feature number:', X_kbest_features.shape[1])

# chi2_features.scores_ adalah nilai chi-square, semakin tinggi nilainya maka semakin baik fiturnya
data_chi2 = pd.DataFrame(chi2_features.scores_, columns=['nilai'])
data_chi2

# Menampilkan fitur beserta nilainya
feature = res_tfidf['tf_idf'][0].get_feature_names_out()
data_chi2['fitur'] = feature
data_chi2

# Mengurutkan fitur terbaik
data_chi2.sort_values(by='nilai', ascending=False)

# Menampilkan mask pada feature yang diseleksi
# False berarti fitur tidak terpilih dan True berarti fitur terpilih
mask = chi2_features.get_support()
mask

# Menampilkan fitur-fitur terpilih berdasarkan mask atau nilai tertinggi yang sudah dikalkulasi pada Chi-Square
new_feature = []
for bool, f in zip(mask, feature):
  if bool:
    new_feature.append(f)
  selected_feature = new_feature
selected_feature

#Save vectorizer.vocabulary_
#Menyimpan vektor dari vocabulary di atas dalam bentuk pickle (.pkl)
pickle.dump(selected_feature,open("/content/drive/MyDrive/Orbit/selected_feature_tf-idf_ngram1.pkl","wb"))

# Menampilkan fitur-fitur yang sudah diseleksi 
# Beserta nilai vektornya pada keseluruhan data untuk dijalankan pada proses machine learning

# Hanya k fitur yang terpilih sesuai parameter k yang ditentukan sebelumnya

data_selected_feature = pd.DataFrame(X_kbest_features, columns=selected_feature)
data_selected_feature

selected_x = X_kbest_features
selected_x

"""# Modelling (Machine Learning)"""

#Import Library
import random
from sklearn.model_selection import train_test_split

#Algoritme
# Stochastic Gradient Descent
from sklearn.linear_model import SGDClassifier

x = selected_x
y = df_new_cut['rating']
#Memisahkan data training dan data testing dengan perbandingan 80:20
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=0)

#print
print('Banyak data x_train :',len(x_train))
print('Banyak data x_test  :',len(x_test))
print('Banyak data y_train :',len(y_train))
print('Banyak data y_test  :',len(y_test))

#Training Model

from datetime import datetime
start_time = datetime.now()
from joblib import dump
#algoritme fitting

text_algorithm = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)

model = text_algorithm.fit(x_train, y_train)
# save the model to disk
dump(model, filename="/content/drive/MyDrive/Orbit/model_sentiment_sgd.joblib")

end_time = datetime.now()
result_time  =end_time-start_time
print("Duration:",result_time)

#Prediksi
predicted = model.predict(x_test)

#Hasil ini akan kita bandingkan dengan nilai y_test (labeling dari manusia)
predicted

"""# Model Evaluation"""

#Library evaluasi
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

#Compute performance manually
NewprediksiBenar = (predicted == y_test).sum()
NewprediksiSalah = (predicted != y_test).sum()
    
print("prediksi benar: ", NewprediksiBenar, " data")
print("prediksi salah: ", NewprediksiSalah, " data")
print("Akurasi Algoritme: ", NewprediksiBenar/(NewprediksiBenar+NewprediksiSalah)*100,"%")

CM = confusion_matrix(y_test,predicted)

TN = CM[0][0]
FN = CM[1][0]
TP = CM[1][1]
FP = CM[0][1]
precision    = TP/(TP+FP)
recall       = TP/(TP+FN)
print ("TRUE NEGATIVE (TN):",TN)
print ("FALSE NEGATIVE (FN):",FN)
print ("TRUE POSITIVE (TP):",TP)
print ("FALSE POSITIVE (FP):",FP)
print ("PRECISION:",precision*100,"%")
print ("RECALL:",recall*100,"%")
print(classification_report(y_test,predicted))

#Menggunakan Cross Validation untuk memvalidasi data

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score

cv        = ShuffleSplit(n_splits=10, test_size=0.2, random_state=2)
accuracy= (cross_val_score(model,x,y,cv=cv,scoring='accuracy'))
score_accuracy = np.mean(accuracy)
print ("accuracy: ",accuracy)
print ("accuracy: ",score_accuracy)

"""# Deployment

karena memakai tf-idf n_gram 1
"""

from joblib import load
#Hasil model
pipeline = load("/content/drive/MyDrive/Orbit/model_sentiment_sgd.joblib")

#preprocessing text
data_input = input("Masukkan sentiment:\n")
data_input = data_preprocessing(data_input)

#Load
#proses TF-IDF dari vocab
tfidf = TfidfVectorizer
loaded_vec = TfidfVectorizer(decode_error="replace", vocabulary=set(pickle.load(open("/content/drive/MyDrive/Orbit/selected_feature_tf-idf_ngram1.pkl", "rb"))))
hasil = pipeline.predict(loaded_vec.fit_transform([data_input]))

#pernyataan
if (hasil==1):
    s ="Rating 1"
elif (hasil==2):
    s ="Rating 2"
elif(hasil==3):
    s ="Rating 3"
elif (hasil==4):
    s ="Rating 4"
else:
   s ="Rating 5"
    
print("Hasil prediksi Rating: ", s)

"""# WordCloud"""

# Import Library WordCloud. WordCloud digunakan untuk melihat secara visual kata-kata yang paling sering muncul.
# Import Library cv2 untuk mengolah gambar menjadi masking WordCloud

import cv2
from wordcloud import WordCloud

# Download gambar masking
!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/img/cloud.jpg

originalImage = cv2.imread('cloud.jpg')
grayImage = cv2.cvtColor(originalImage, cv2.COLOR_BGR2GRAY)
(thresh, cloud_mask) = cv2.threshold(grayImage, 100, 255, cv2.THRESH_BINARY)

# WordCloud Label rating 1

kata = df_new_cut[df_new_cut.rating == 1]
rating_string = []

for t in kata.cleanReview:
  rating_string.append(t)

normal_string = pd.Series(rating_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(normal_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# WordCloud Label rating 2

kata = df_new_cut[df_new_cut.rating == 2]
rating_string = []

for t in kata.cleanReview:
  rating_string.append(t)

normal_string = pd.Series(rating_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(normal_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# WordCloud Label rating 3

kata = df_new_cut[df_new_cut.rating == 3]
rating_string = []

for t in kata.cleanReview:
  rating_string.append(t)

normal_string = pd.Series(rating_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(normal_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# WordCloud Label rating 4

kata = df_new_cut[df_new_cut.rating == 4]
rating_string = []

for t in kata.cleanReview:
  rating_string.append(t)

normal_string = pd.Series(rating_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(normal_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

# WordCloud Label rating 5

kata = df_new_cut[df_new_cut.rating == 5]
rating_string = []

for t in kata.cleanReview:
  rating_string.append(t)

normal_string = pd.Series(rating_string).str.cat(sep=' ')
from wordcloud import WordCloud

wordcloud = WordCloud(width=1600, height=800, margin=10,
                      background_color='white', colormap='Dark2',
                      max_font_size=200, min_font_size=25,
                      mask=cloud_mask, contour_width=10, contour_color='firebrick',
                      max_words=100).generate(normal_string)
plt.figure(figsize=(10,8))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

"""# Resume

setelah melakukan text preprocessing dan feature enginnering kemudian dilakukan modelling data , dengan train dan test dibagi 80 % : 20 % kemudian tetapi perlu dingat train ini data rating 5 terlalu banyak dibangding dengan rating lainnya, fitur merupakan review dari pembeli dan target yaitu rating yang digunakan , kemudian dilakukan pemodelan, setelah dilakukan melakukan evaluasi report matrix, jumlah salah dan benar dalam redict, dan juga cross validasi, setelah itu dilakukan deployment model terhadap data inputan baru sehingga dapat di kategorikan mana yang review sentiment pembeli masuk ke kategori rating berapa.

terakhir tidak lupa membuat wordcloud berdasarkan rating pembeli.
dari beberapa proses modelling saya gunakan dengan beberapa algoritma kalsifikasi seperti KNN,SGD, Naive bayes, MLP, random forest,dan decision tree. Saya memilih SGD karena prosesnya cepat dan nilai akurasi lebih baik daripada algoritma klasifikasi lainnya. dengan averange acc 79,5%.
"""