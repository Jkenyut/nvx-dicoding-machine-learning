# -*- coding: utf-8 -*-
"""Dicoding Time Series Intermedite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17xuDCvSZWvFVhC1f3plEW8HlH7sAVlPa

## Satria Nur Saputro
# Dicoding
# Dataset [Click](https://www.kaggle.com/code/amirrezaeian/time-series-data-analysis-using-lstm-tutorial/data)
"""

!wget "https://storage.googleapis.com/kaggle-data-sets/122/260/compressed/household_power_consumption.txt.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20220911%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20220911T090752Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=2a2a824adfe10ea967033832e78a0c481ec3887a9824cfe3398e3fc98a02478a6de31612785f314948b91f289d650cc8ced4b3860ae0c3fbe663afec901342fc414c0928155a131a9b053af72520a71cb772fd2e98e359dff160e53500809fa7e0d8889e6bc71f94c7b88d76cd35d4710dbf9c6a6583eec87e152d37a719192a79b411c8e34a236bbfa21f23300e592dfeb491543d5c7cabad853d85b078de5136c39a869619227cbf8dc6a1feb9b2c80b45d355313a185e87069c48a2d1fcb504c08fb2b2834825b5ed4a8ccae482145456e2e478d5bf34c55e7810864cac3aef68582d1d97f4a2139df1cf89912b490cae4bcae660f0f5d98f1cdc141a5e6d"

"""ubah nama file download menjadi data.zip"""

!unzip "data.zip"

"""Import library"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('/content/household_power_consumption.txt', sep=';', 
                 parse_dates={'dt' : ['Date', 'Time']},na_values=['nan','?'], infer_datetime_format=True, index_col='dt')
dataset.head()

"""ukuran dataset 2075259 baris dan 7 kolom"""

dataset.shape

"""cek data kosong"""

dataset.isnull().sum()

"""replace data kosong ke rata rata kolom"""

dataset['Global_active_power'] = dataset['Global_active_power'].fillna(dataset['Global_active_power'].mean())
dataset['Global_reactive_power'] = dataset['Global_reactive_power'].fillna(dataset['Global_reactive_power'].mean())
dataset['Voltage'] = dataset['Voltage'].fillna(dataset['Voltage'].mean())
dataset['Global_intensity'] = dataset['Global_intensity'].fillna(dataset['Global_intensity'].mean())
dataset['Sub_metering_1'] = dataset['Sub_metering_1'].fillna(dataset['Sub_metering_1'].mean())
dataset['Sub_metering_2'] = dataset['Sub_metering_2'].fillna(dataset['Sub_metering_2'].mean())
dataset['Sub_metering_3'] = dataset['Sub_metering_3'].fillna(dataset['Sub_metering_3'].mean())

"""cek kembali dataset ksosong"""

dataset.isnull().sum()

"""info dataset"""

dataset.info()

"""deskrpsi dataset"""

dataset.describe()

def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
	n_vars = 1 if type(data) is list else data.shape[1]
	dff = pd.DataFrame(data)
	cols, names = list(), list()
	# input sequence (t-n, ... t-1)
	for i in range(n_in, 0, -1):
		cols.append(dff.shift(i))
		names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]
	# forecast sequence (t, t+1, ... t+n)
	for i in range(0, n_out):
		cols.append(dff.shift(-i))
		if i == 0:
			names += [('var%d(t)' % (j+1)) for j in range(n_vars)]
		else:
			names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]
	# put it all together
	agg = pd.concat(cols, axis=1)
	agg.columns = names
	# drop rows with NaN values
	if dropnan:
		agg.dropna(inplace=True)
	return agg

dates = dataset.index
feature = dataset[['Global_active_power'	,'Global_reactive_power'	,'Voltage'	,'Global_intensity'	,'Sub_metering_1',	'Sub_metering_2'	,'Sub_metering_3']].values

"""# Normalisasi Menggunakan MinMax Scaler"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
scaled = scaler.fit_transform(feature)

"""# Split dataset Train Test 80:20"""

dataset_pembaruan = series_to_supervised(scaled, 1, 1)


dataset_pembaruan.drop(dataset_pembaruan.columns[[8,9,10,11,12,13]], axis=1, inplace=True)
values = dataset_pembaruan.values
train = values[:values.shape[0] * 80, :]
test = values[values.shape[0] * 80:, :]

X_train, y_train = train[:, :-1], train[:, -1]
X_test, y_test = test[:, :-1], test[:, -1]

"""# reshape tarining testing"""

X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Bidirectional, TimeDistributed, LSTM, Dropout
model = Sequential()

model = Sequential()
model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1))

"""# Optimizer and loss"""

optimizers = keras.optimizers.Adam(1e-3)
model.compile(loss='mae',
              optimizer=optimizers,
              metrics=['accuracy','mae'])

"""# callback"""

class callback(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('loss') < 0.09):
      self.model.stop_training = True

callbacks = callback()
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',mode="max", patience=3)

"""# Training data"""

num_epochs = 200
history = model.fit(train_X, train_y, epochs=num_epochs, validation_data=(test_X, test_y), callbacks=[callbacks,early_stopping], batch_size=4,verbose=2)

"""# Plot akurasi dan loss"""

import matplotlib.pyplot as plt

x = [0,1]
y = [0,history.history['accuracy'][0]]
plt.plot(x, y)
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

x_mae = [0,history.history['mae'][0]]
y = [0,1]
plt.plot(x_mae,y)
plt.title('Loss Model')
plt.ylabel('loss mae')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()